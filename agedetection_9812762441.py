# -*- coding: utf-8 -*-
"""AgeDetection-9812762441.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dDkbo1YzDhNNVmP3NCRQzXjxu7wX4hQt

*   Neural Netwroks & Deep Learning - Dr Ghiasi

*   Mohammad Parsa Etemadheravi
*   9812762441
"""

from google.colab import drive
drive.mount('/content/drive')

"""In this exercise we tend to predict the age of individuals with respect to the image of their face by transfer learnign."""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
from torch.autograd import Variable
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
from PIL import Image
from tempfile import TemporaryDirectory
from torchvision import models

# Transform on data
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        transforms.ColorJitter(),
        transforms.RandomVerticalFlip()
           ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

    ]),
}

# getting data
data_dir = '/content/drive/My Drive/NN-Course/AgeDetectionDataset'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# class NeuralNetwork(nn.Module) :

#   def __init__(self):
#     super().__init__()

# #     write your code

#   def forward(self, x ):
# #     write your code

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    since = time.time()

    # Create a temporary directory to save training checkpoints
    with TemporaryDirectory() as tempdir:
        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')

        torch.save(model.state_dict(), best_model_params_path)
        best_acc = 0.0

        for epoch in range(num_epochs):
            print(f'Epoch {epoch}/{num_epochs - 1}')
            print('-' * 10)

            # Each epoch has a training and validation phase
            for phase in ['train', 'val']:
                if phase == 'train':
                    model.train()  # Set model to training mode
                else:
                    model.eval()   # Set model to evaluate mode

                running_loss = 0.0
                running_corrects = 0

                # Iterate over data.
                for inputs, labels in dataloaders[phase]:
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    # zero the parameter gradients
                    optimizer.zero_grad()

                    # forward
                    # track history if only in train
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)

                        # backward + optimize only if in training phase
                        if phase == 'train':
                            loss.backward()
                            optimizer.step()

                    # statistics
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
                if phase == 'train':
                    scheduler.step()

                epoch_loss = running_loss / dataset_sizes[phase]
                epoch_acc = running_corrects.double() / dataset_sizes[phase]

                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

                # deep copy the model
                if phase == 'val' and epoch_acc > best_acc:
                    best_acc = epoch_acc
                    torch.save(model.state_dict(), best_model_params_path)

            print()

        time_elapsed = time.time() - since
        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
        print(f'Best val Acc: {best_acc:4f}')

        # load best model weights
        model.load_state_dict(torch.load(best_model_params_path))
    return model

"""## **Implementing VGG16:** Best Validation Accuracy for 10 epochs: **32%**

"""

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

class TransferLearningModel(nn.Module):
    def __init__(self, vgg16_model, mlp_model):
        super(TransferLearningModel, self).__init__()
        self.vgg16 = vgg16_model
        self.mlp = mlp_model

        # Remove the last fully connected layer of VGG16
        vgg16_classifier = list(self.vgg16.classifier.children())[:-1]
        self.vgg16.classifier = nn.Sequential(*vgg16_classifier)
        mlp_input_size = 4096  # Adjust this based on the VGG16 classifier's last layer output size
        self.mlp.fc1 = nn.Linear(mlp_input_size, hidden_size)

    def forward(self, x):
        vgg16_features = self.vgg16.features(x)
        vgg16_avgpool = self.vgg16.avgpool(vgg16_features)
        # print(x.size(0))
        vgg16_flattened = vgg16_avgpool.view(x.size(0), -1)
        # vgg16_flattened = F.relu(nn.Linear(512 * 7 * 7, hidden_size)(vgg16_flattened))
        vgg16_output = self.vgg16.classifier(vgg16_flattened)

        # Forward pass through the adjusted MLP
        mlp_output = self.mlp(vgg16_output)
        combined_output = torch.cat((vgg16_output, mlp_output), dim=1)
        return combined_output

hidden_size = 1000
num_classes = 5



# Create instances of the models
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


vgg16_model = models.vgg16(pretrained = 'IMAGENET1K_V1')
mlp_model = NeuralNetwork(input_size=hidden_size, hidden_size=hidden_size, num_classes=num_classes)
transfer_learning_model = TransferLearningModel(vgg16_model, mlp_model)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
transfer_learning_model = transfer_learning_model.to(device)

loss = nn.CrossEntropyLoss()
optimizer = optim.SGD(params=transfer_learning_model.parameters(), lr=0.001)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

train_model(transfer_learning_model, loss, optimizer, exp_lr_scheduler, num_epochs=10)

"""## **Implementing RESNet18:** Best Validation Accuracy for 10 epochs: **40%**

"""

class ModifiedResNet18(nn.Module):
    def __init__(self, num_classes):
        super(ModifiedResNet18, self).__init__()
        resnet18_model = models.resnet18(pretrained=True)

        # Remove the original fully connected layer
        self.resnet18 = nn.Sequential(*list(resnet18_model.children())[:-1])

    def forward(self, x):
        x = self.resnet18(x)
        x = x.view(x.size(0), -1)
        return x

class TransferLearningModel(nn.Module):
    def __init__(self, resnet18_model, mlp_model):
        super(TransferLearningModel, self).__init__()
        self.resnet18 = resnet18_model
        self.mlp = mlp_model

    def forward(self, x):
        x = self.resnet18(x)
        x = self.mlp(x)
        return x

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

input_size = 512  # ResNet18 outputs 512 features
hidden_size = 256
num_classes = 5


resnet18_model = ModifiedResNet18(num_classes=num_classes)
mlp_model = NeuralNetwork(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)
transfer_learning_model = TransferLearningModel(resnet18_model, mlp_model)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
transfer_learning_model = transfer_learning_model.to(device)

loss = nn.CrossEntropyLoss()
optimizer = optim.SGD(params=transfer_learning_model.parameters(), lr=0.001)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

train_model(transfer_learning_model, loss, optimizer, exp_lr_scheduler, num_epochs=10)

"""## **Implementing RESNet50:** Best Validation Accuracy for 10 epochs: **24%**"""

class ModifiedResNet50(nn.Module):
    def __init__(self, num_classes):
        super(ModifiedResNet50, self).__init__()
        # Load pre-trained ResNet50 model
        resnet50_model = models.resnet50(pretrained=True)

        # Remove the original fully connected layer
        self.resnet50 = nn.Sequential(*list(resnet50_model.children())[:-1])

    def forward(self, x):
        x = self.resnet50(x)
        x = x.view(x.size(0), -1)
        return x

class TransferLearningModel(nn.Module):
    def __init__(self, resnet50_model, mlp_model):
        super(TransferLearningModel, self).__init__()
        self.resnet50 = resnet50_model
        self.mlp = mlp_model

    def forward(self, x):
        x = self.resnet50(x)
        x = self.mlp(x)
        return x

input_size = 2048  # ResNet50 outputs 2048 features
hidden_size = 256  # Adjust as needed
num_classes = 5  # Adjust based on the number of age classes

# Create instances of the models
resnet50_model = ModifiedResNet50(num_classes=num_classes)
mlp_model = NeuralNetwork(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)
transfer_learning_model = TransferLearningModel(resnet50_model, mlp_model)

# Move the model to the GPU if available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
transfer_learning_model = transfer_learning_model.to(device)

# Define the loss function, optimizer, and learning rate scheduler
loss = nn.CrossEntropyLoss()
optimizer = optim.SGD(params=transfer_learning_model.parameters(), lr=0.001)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

# Train the model using the provided training function
train_model(transfer_learning_model, loss, optimizer, exp_lr_scheduler, num_epochs=10)